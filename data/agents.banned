#--
# Ruwiki
#   Copyright © 2002 - 2004, Digikata and HaloStatue
#   Alan Chen (alan@digikata.com)
#   Austin Ziegler (ruwiki@halostatue.ca)
#
# Licensed under the same terms as Ruby.
#
# $Id$
#
# This file contains the list of User Agent strings which will be greeted
# with "403 Forbidden" responses by Ruwiki. These are generally known email
# harvesters or link directory builders for reciprocal link partners. These
# may also be robots that routinely ignore the robots.txt file.
#
# This list is generated from a wide variety of sources including:
# * http://www.clockwatchers.com/robots_list.html
# * http://searchenginewatch.com/webmasters/article.php/2167991
# * http://www.neilgunton.com/spambot_trap/
# * http://www.robotstxt.org/wc/active/all.txt
#
# This file is in "extended" regular expression format, one optional
# expression to a line. Spaces are not significant and comments are
# allowed. If you want to recognise a space in your regular expression, do
# so either with a character class ([ ]) or the whitespace meta-character
# (\s). Hash marks must be escaped (\#) or they will be treated as comment
# markers. Blank or comment-only lines are ignored. All other lines will
# be joined together:
#
#       foo
#       bar
#
# becomes:
#
#       %r{foo|bar}x
#++

  # Known email harvesters
(?i:^nicerspro)
(?i:^teleport)
^CherryPicker
^Crescent # Crescent Internet ToolPak
^EmailCollector
^EmailSiphon
^EmailWolf
^ExtractorPro
^Microsoft\sURL
^WebEMailExtrac

  # Link directory builders.
^LinkWalker
^Zeus.*Webster

  # Strongly suspected spoofed user agents from spam harvesters. These user
  # agents have been reported patterns by other antispam fighters.
^[A-Z]+$
^Internet\sExplore\s5.x
^Mozilla.*NEWT
^Mozilla\/4.0$
MSIECrawler
